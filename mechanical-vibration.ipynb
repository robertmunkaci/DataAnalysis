{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install sklearn\n",
    "%pip install xgboost\n",
    "%pip install lightgbm\n",
    "%pip install scikit-optimize\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import skopt as so\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Window calc function\n",
    "def add_rolling_features(df, state_col, load_col, speed_col, sensors, window_sizes):\n",
    "    \"\"\"\n",
    "    Calculates rolling statistics for each sensor in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - state_col: The column containing the state of the machine.\n",
    "    - load_col: The column containing the load on the machine.\n",
    "    - speed_col: The column containing the speed of the machine.\n",
    "    - sensors: A list of columns containing sensor data.\n",
    "    - window_sizes: A list of window sizes to calculate statistics for.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with the rolling statistics added as new columns.\n",
    "    \"\"\"\n",
    "    # Define statistics to calculate\n",
    "    stats = ['mean', 'min', 'max', 'median', 'std']\n",
    "\n",
    "    # Iterate over each sensor and each window size\n",
    "    for sensor in sensors:\n",
    "        for window in window_sizes:\n",
    "            # Group by specified columns and apply rolling functions\n",
    "            grouped = df.groupby([state_col, load_col, speed_col])[sensor]\n",
    "            for stat in stats:\n",
    "                # Construct new column name\n",
    "                new_col_name = f'{sensor}_{stat}{window}'\n",
    "                if stat == 'mean':\n",
    "                    df[new_col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                elif stat == 'min':\n",
    "                    df[new_col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "                elif stat == 'max':\n",
    "                    df[new_col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "                elif stat == 'median':\n",
    "                    df[new_col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "                elif stat == 'std':\n",
    "                    df[new_col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_and_optimize_model(X, y, model, param_grid, cross_val, num_iter, sc, num_jobs, rnd_state):\n",
    "    \"\"\"\n",
    "    Trains a machine learning model using grid search with cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - X, y: Training data and labels.\n",
    "    - model: Uninitialized model that supports scikit-learn interface.\n",
    "    - param_grid: Grid of parameters to tune.\n",
    "    - cv: Cross-validation strategy.\n",
    "    - scoring: Metric to evaluate the models.\n",
    "    - n_jobs: Number of jobs to run in parallel.\n",
    "    - random_state: Seed for random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing:\n",
    "      - 'best_model': The best model from grid search.\n",
    "      - 'best_score': The highest score achieved.\n",
    "      - 'best_params': The parameters for the best model.\n",
    "      - 'cv_results': Detailed results from all the training iterations.\n",
    "    \"\"\"\n",
    "    # Setup BayesSearchCV\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator = model,\n",
    "        search_spaces = param_grid,\n",
    "        scoring = sc,\n",
    "        n_iter = num_iter, # Number of iterations to run\n",
    "        cv = cross_val,      # 3-fold cross-validation\n",
    "        n_jobs = num_jobs, # Use all available cores\n",
    "        random_state = rnd_state,\n",
    "    )\n",
    "    bayes_search.fit(X, y)\n",
    "\n",
    "    return {\n",
    "        'best_model': bayes_search.best_estimator_,\n",
    "        'best_score': bayes_search.best_score_,\n",
    "        'best_params': bayes_search.best_params_,\n",
    "        'cv_results': bayes_search.cv_results_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# Load datasets\n",
    "missing_tooth = pd.read_csv('missing_tooth.csv')\n",
    "tooth_chipped_fault = pd.read_csv('tooth_chipped_fault.csv')\n",
    "surface_fault = pd.read_csv('surface_fault.csv')\n",
    "no_fault = pd.read_csv('no_fault.csv')\n",
    "root_crack = pd.read_csv('root_crack.csv')\n",
    "eccentricity = pd.read_csv('eccentricity.csv')\n",
    "\n",
    "# Concatenate datasets\n",
    "df = pd.concat([\n",
    "    no_fault,\n",
    "    missing_tooth,\n",
    "    tooth_chipped_fault,\n",
    "    surface_fault,\n",
    "    root_crack,\n",
    "    eccentricity\n",
    "])\n",
    "\n",
    "# Transform gear_fault_desc for easier handling, and drop unnecessary columns, transform time_x to datetime and normalize time\n",
    "# Create new dataset to work with and addjust\n",
    "df_work = df.copy()\n",
    "df_work['time_x'] = pd.to_datetime(df['time_x'])\n",
    "df_work['time_normalized'] = df_work.groupby(['gear_fault_desc', 'load_value', 'speedSet'])['time_x'].transform(lambda x: (x - x.min()).dt.total_seconds())\n",
    "df_work = df_work.drop(columns=['time_x'])\n",
    "df_work['state'] = df_work.gear_fault_desc.apply(lambda x: ['No fault', 'missing tooth', 'chipped tooth', 'surface defect', 'Root crack', 'eccentricity'].index(x))\n",
    "df_work = df_work.drop(columns=['gear_fault_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for stationarity showed it is not stationary so I will split the data into first 80% for training and last 20% for testing\n",
    "X_train = df_work[df_work['time_normalized'] < 4].copy()\n",
    "X_test = df_work[df_work['time_normalized'] >= 4].copy()\n",
    "\n",
    "# I will calculate the rolling 25, 50, 100, 250, 500, 1000 mean, min, max, median and standard deviation \n",
    "# for sensor1 and sensor2 and use it to create a new features for every combination of state, load_value and speedSet\n",
    "window_sizes = [25, 50, 100, 250, 500, 1000]\n",
    "X_train = add_rolling_features(X_train, 'state', 'load_value', 'speedSet', ['sensor1', 'sensor2'], window_sizes)\n",
    "X_test = add_rolling_features(X_test, 'state', 'load_value', 'speedSet', ['sensor1', 'sensor2'], window_sizes)\n",
    "\n",
    "# X_train and X_test have NaN values that I will fill with 0, \n",
    "# delete time_normalized column as in a practical feature in this case\n",
    "# and standardize the data\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "Y_train = X_train['state']\n",
    "Y_test = X_test['state']\n",
    "X_train = X_train.drop(columns=['state', 'time_normalized'])\n",
    "X_test = X_test.drop(columns=['state', 'time_normalized'])\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and hyperparameters\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(0.01, 0.5, 'log-uniform'),\n",
    "    'n_estimators': Integer(50, 1000),\n",
    "    'num_leaves': Integer(20, 40),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'min_child_weight': Real(0.001, 10, 'log-uniform'),\n",
    "    'colsample_bytree': Real(0.1, 1.0, 'uniform'),\n",
    "    'subsample': Real(0.5, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "# Define models\n",
    "model = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', random_state=0)\n",
    "cv=4\n",
    "n_iter=32\n",
    "scoring='accuracy'\n",
    "n_jobs=-1\n",
    "random_state=0\n",
    "# Train and optimize model\n",
    "result = train_and_optimize_model(X_train, Y_train, model, search_spaces, cv, n_iter, scoring, n_jobs, random_state)\n",
    "# The training can be easilly appliead for other models, just change or add the model and the search_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['best_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['cv_results'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
