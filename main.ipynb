{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from plotnine import *\n",
    "\n",
    "# Load the datasets\n",
    "missing_tooth = pd.read_csv('missing_tooth.csv')\n",
    "tooth_chipped = pd.read_csv('tooth_chipped_fault.csv')\n",
    "surface_fault = pd.read_csv('surface_fault.csv')\n",
    "no_fault = pd.read_csv('no_fault.csv')\n",
    "root_crack = pd.read_csv('root_crack.csv')\n",
    "eccentricity = pd.read_csv('eccentricity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_tooth['state'] = 'missing_tooth'\n",
    "tooth_chipped['state'] = 'tooth_chipped'\n",
    "surface_fault['state'] = 'surface_fault'\n",
    "no_fault['state'] = 'no_fault'\n",
    "root_crack['state'] = 'root_crack'\n",
    "eccentricity['state'] = 'eccentricity'\n",
    "\n",
    "df = pd.concat([\n",
    "    missing_tooth, \n",
    "    tooth_chipped,\n",
    "    surface_fault,\n",
    "    no_fault,\n",
    "    root_crack, \n",
    "    eccentricity\n",
    "])\n",
    "display(df.info())\n",
    "#Normalize time_x grouped by state, load_value, speedSet\n",
    "df['time_x'] = pd.to_datetime(df['time_x'])\n",
    "df['time_normalized'] = df.groupby(['state', 'load_value', 'speedSet'])['time_x'].transform(lambda x: (x - x.min()).dt.total_seconds())\n",
    "\n",
    "df['time_normalized']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='time_normalized').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_formatted'] = pd.to_datetime(df.time_x, format=\"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sensor1_max_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().max())\n",
    "df['sensor1_min_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().min())\n",
    "\n",
    "df['sensor2_max_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().max())\n",
    "df['sensor2_min_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().min())\n",
    "\n",
    "df['sensor1_mean_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "df['sensor2_mean_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "\n",
    "df['sensor1_std_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().std())\n",
    "df['sensor2_std_exp'] = df.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables before using them\n",
    "speedload_8_0 = df[(df['speedSet'] == 8.33203125) & (df['load_value'] == 0)].copy()\n",
    "speedload_8_80 = df[(df['speedSet'] == 8.33203125) & (df['load_value'] == 80)].copy()\n",
    "speedload_25_0 = df[(df['speedSet'] == 25) & (df['load_value'] == 0)].copy()\n",
    "speedload_25_80 = df[(df['speedSet'] == 25) & (df['load_value'] == 80)].copy()\n",
    "speedload_40_0 = df[(df['speedSet'] == 40) & (df['load_value'] == 0)].copy()\n",
    "speedload_40_80 = df[(df['speedSet'] == 40) & (df['load_value'] == 80)].copy()\n",
    "\n",
    "# Calculate the avarage column for sensor1 and sensor2 values in time for speedLoad 8.33 and 0 and then the other combinations\n",
    "speedload_8_0['sensor1_avg'] = speedload_8_0.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_8_0['sensor2_avg'] = speedload_8_0.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "speedload_8_80['sensor1_avg'] = speedload_8_80.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_8_80['sensor2_avg'] = speedload_8_80.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "speedload_25_0['sensor1_avg'] = speedload_25_0.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_25_0['sensor2_avg'] = speedload_25_0.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "speedload_25_80['sensor1_avg'] = speedload_25_80.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_25_80['sensor2_avg'] = speedload_25_80.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "speedload_40_0['sensor1_avg'] = speedload_40_0.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_40_0['sensor2_avg'] = speedload_40_0.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n",
    "speedload_40_80['sensor1_avg'] = speedload_40_80.groupby(['state', 'load_value', 'speedSet']).sensor1.transform(lambda x: x.expanding().mean())\n",
    "speedload_40_80['sensor2_avg'] = speedload_40_80.groupby(['state', 'load_value', 'speedSet']).sensor2.transform(lambda x: x.expanding().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the KPSS test to each unique state for 'sensor1' values by 'normalized_time'.\n",
    "def apply_kpss(series):\n",
    "    kpss_stat, p_value, lags, crit = kpss(series, 'c')\n",
    "    return kpss_stat, p_value, crit\n",
    "\n",
    "# Apply the KPSS test to all unique states\n",
    "unique_states = speedload_8_0['state'].unique()\n",
    "kpss_results = {}\n",
    "for state in unique_states:\n",
    "    # Extracting the time series data for 'sensor1' for the current state\n",
    "    time_series = speedload_8_0.loc[speedload_8_0['state'] == state, 'sensor1']\n",
    "    # Applying KPSS test\n",
    "    kpss_stat, p_value, crit = apply_kpss(time_series)\n",
    "    kpss_results[state] = (kpss_stat, p_value, crit)\n",
    "\n",
    "# You can print out the results or convert it to a DataFrame\n",
    "kpss_results_df = pd.DataFrame(kpss_results, index=['KPSS Statistic', 'p-value', 'Critical Values']).T\n",
    "print(kpss_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define your custom color palette\n",
    "colors = ['#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00', '#FFFF33']\n",
    "\n",
    "# Plot the sensor1_avg values then sensor2_avg values in time for every speedSet and load_value combination in seperate plots for each state\n",
    "display(ggplot(data = speedload_8_0, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 8.33, záťaž 0'))\n",
    "display(ggplot(data = speedload_8_0, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 8.33, záťaž 0'))\n",
    "display(ggplot(data = speedload_8_80, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 8.33, záťaž 80'))\n",
    "display(ggplot(data = speedload_8_80, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 8.33, záťaž 80'))\n",
    "display(ggplot(data = speedload_25_0, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 25, záťaž 0'))\n",
    "display(ggplot(data = speedload_25_0, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 25, záťaž 0'))\n",
    "display(ggplot(data = speedload_25_80, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 25, záťaž 80'))\n",
    "display(ggplot(data = speedload_25_80, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 25, záťaž 80'))\n",
    "display(ggplot(data = speedload_40_0, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 40, záťaž 0'))\n",
    "display(ggplot(data = speedload_40_0, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 40, záťaž 0'))\n",
    "display(ggplot(data = speedload_40_80, mapping = aes(x = 'time_normalized', y = 'sensor1_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 40, záťaž 80'))\n",
    "display(ggplot(data = speedload_40_80, mapping = aes(x = 'time_normalized', y = 'sensor2_avg', color = 'state')) + geom_line() + ggtitle('Rýchlosť 40, záťaž 80'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "display(ggplot(data=speedload_8_0, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() + \n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 8.33, záťaž 0') + xlim(2.417, 2.44))\n",
    "display(ggplot(data=speedload_8_80, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 8.33, záťaž 80') + xlim(2.417, 2.44))\n",
    "display(ggplot(data=speedload_25_0, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 25, záťaž 0') + xlim(2.4, 2.45))\n",
    "display(ggplot(data=speedload_25_80, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 25, záťaž 80') + xlim(2.4, 2.45))\n",
    "display(ggplot(data=speedload_40_0, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 40, záťaž 0') + xlim(2.4, 2.45))\n",
    "display(ggplot(data=speedload_40_80, mapping=aes(x='sensor2', fill='state')) + geom_density(alpha=0.3) + stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 40, záťaž 80') + xlim(2.37, 2.55))\n",
    "\n",
    "display(ggplot(data = speedload_8_80, mapping = aes(x = 'sensor2', fill = 'state')) + geom_density(alpha = 0.3)+ stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 8.33, záťaž 80'))\n",
    "display(ggplot(data = speedload_25_0, mapping = aes(x = 'sensor2', fill = 'state')) + geom_density(alpha = 0.3)+ stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 25, záťaž 0'))\n",
    "display(ggplot(data = speedload_25_80, mapping = aes(x = 'sensor2', fill = 'state')) + geom_density(alpha = 0.3)+ stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 25, záťaž 80'))\n",
    "display(ggplot(data = speedload_40_0, mapping = aes(x = 'sensor2', fill = 'state')) + geom_density(alpha = 0.3)+ stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 40, záťaž 0'))\n",
    "display(ggplot(data = speedload_40_80, mapping = aes(x = 'sensor2', fill = 'state')) + geom_density(alpha = 0.3)+ stat_ecdf() +\n",
    "        scale_fill_manual(values=colors) + ggtitle('Rýchlosť 40, záťaž 80'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "display(ggplot(data = df, mapping = aes(x = 'sensor1', y = 'sensor2', color = 'no_fault'))\n",
    "         + geom_point(size = 0.1, alpha = 0.5) + facet_wrap('~no_fault'))\n",
    "display(ggplot(data = df, mapping = aes(x = 'sensor1_std_exp', y = 'sensor2_std_exp', color = 'no_fault'))\n",
    "         + geom_point(size = 0.1, alpha = 0.5) + facet_wrap('~no_fault'))\n",
    "display(ggplot(data = df, mapping = aes(x = 'sensor1_mean_exp', y = 'sensor2_mean_exp', color = 'no_fault'))\n",
    "         + geom_point(size = 0.1, alpha = 0.5) + facet_wrap('~no_fault'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save DataFrames to an HDF5 file\n",
    "with pd.HDFStore('speedload_data.h5', mode='w') as store:\n",
    "    store.put('speedload_8_0', speedload_8_0, format='table')\n",
    "    store.put('speedload_8_80', speedload_8_80, format='table')\n",
    "    store.put('speedload_25_0', speedload_25_0, format='table')\n",
    "    store.put('speedload_25_80', speedload_25_80, format='table')\n",
    "    store.put('speedload_40_0', speedload_40_0, format='table')\n",
    "    store.put('speedload_40_80', speedload_40_80, format='table')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaN values and replace with 0\n",
    "df.fillna(0, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create the graph above only with no_fault vs other states\n",
    "display(ggplot(data = df[df.state.isin(['no_fault', 'missing_tooth'])], mapping = aes(x = 'sensor2_std_exp', y = 'sensor1_std_exp')) + geom_point(aes(color = 'state')))\n",
    "display(ggplot(data = df[df.state.isin(['no_fault', 'tooth_chipped'])], mapping = aes(x = 'sensor2_std_exp', y = 'sensor1_std_exp')) + geom_point(aes(color = 'state')))\n",
    "display(ggplot(data = df[df.state.isin(['no_fault', 'surface_fault'])], mapping = aes(x = 'sensor2_std_exp', y = 'sensor1_std_exp')) + geom_point(aes(color = 'state')))\n",
    "display(ggplot(data = df[df.state.isin(['no_fault', 'root_crack'])], mapping = aes(x = 'sensor2_std_exp', y = 'sensor1_std_exp')) + geom_point(aes(color = 'state')))\n",
    "display(ggplot(data = df[df.state.isin(['no_fault', 'eccentricity'])], mapping = aes(x = 'sensor2_std_exp', y = 'sensor1_std_exp')) + geom_point(aes(color = 'state')))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a formated column of state as Expected: [0 1 2 3 4 5] input\n",
    "display(df.state.unique())\n",
    "df['state_expected'] = df.state.apply(lambda x: ['missing_tooth', 'tooth_chipped', 'surface_fault', 'no_fault', 'root_crack', 'eccentricity'].index(x))\n",
    "# Save the transformation keys for state_expected to later reverse the transformation\n",
    "mapping_dict = df.set_index('state_expected')['state'].to_dict()\n",
    "display(mapping_dict)\n",
    "state_expected_keys = pd.DataFrame(list(mapping_dict.items()), columns=['state_expected', 'state'])\n",
    "display(state_expected_keys)\n",
    "# Create a df dataset that excludes time_x, gear_fault_desc, state, time_formatted\n",
    "df_edit = df[['sensor1', 'sensor2', 'speedSet', 'load_value', 'state_expected', 'time_normalized']]\n",
    "display(df.state_expected.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create if not already created, the database file for appending the model name, validation scores, hyperparameters and train/test/validation distribution\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS model_results\n",
    "                (model_name text, validation_score real, hyperparameters text, train_test_val text)''')\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "total_size = len(df_edit)\n",
    "chunk_size = int(total_size * 0.0001)  # 0.001% of data\n",
    "num_chunks_train = int(total_size * 0.8 / chunk_size)\n",
    "num_chunks_val = int(total_size * 0.1 / chunk_size)\n",
    "num_chunks_test = int(total_size * 0.1 / chunk_size)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df_shuffled = df_edit.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Function to stratify and split the data into chunks\n",
    "def stratified_chunk_split(dfs, num_chunks):\n",
    "    # List to store chunks\n",
    "    chunks = []\n",
    "    \n",
    "    # Get stratified samples\n",
    "    for _, group_data in dfs.groupby('state_expected'):\n",
    "        group_chunks = np.array_split(group_data, num_chunks)\n",
    "        chunks.extend(group_chunks)\n",
    "        \n",
    "    # Shuffle chunks to mix classes\n",
    "    np.random.shuffle(chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Stratify and split into chunks\n",
    "all_chunks = stratified_chunk_split(df_shuffled, num_chunks_train + num_chunks_val + num_chunks_test)\n",
    "\n",
    "# Separate chunks into train, validation, and test sets\n",
    "train_chunks = all_chunks[:num_chunks_train]\n",
    "val_chunks = all_chunks[num_chunks_train:num_chunks_train + num_chunks_val]\n",
    "test_chunks = all_chunks[num_chunks_train + num_chunks_val:]\n",
    "\n",
    "# Combine chunks back into DataFrames\n",
    "X_train = pd.concat(train_chunks, ignore_index=True)\n",
    "X_test = pd.concat(val_chunks, ignore_index=True)\n",
    "X_val = pd.concat(test_chunks, ignore_index=True)\n",
    "\n",
    "y_train = X_train['state_expected']\n",
    "y_test = X_test['state_expected']\n",
    "y_val = X_val['state_expected']\n",
    "\n",
    "X_train['sensor1_max'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_train['sensor1_min'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_train['sensor1_mean'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_train['sensor1_std'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "X_train['sensor2_max'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_train['sensor2_min'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_train['sensor2_mean'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_train['sensor2_std'] = X_train.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "X_test['sensor1_max'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_test['sensor1_min'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_test['sensor1_mean'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_test['sensor1_std'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "X_test['sensor2_max'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_test['sensor2_min'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_test['sensor2_mean'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_test['sensor2_std'] = X_test.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "X_val['sensor1_max'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_val['sensor1_min'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_val['sensor1_mean'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_val['sensor1_std'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor1'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "X_val['sensor2_max'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).max())\n",
    "X_val['sensor2_min'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).min())\n",
    "X_val['sensor2_mean'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).mean())\n",
    "X_val['sensor2_std'] = X_val.groupby(['state_expected', 'load_value', 'speedSet'])['sensor2'].transform(lambda x: x.rolling(window=500, min_periods=1).std())\n",
    "\n",
    "\n",
    "# Drop column state_expected\n",
    "X_train.drop(columns='state_expected', inplace=True)\n",
    "X_test.drop(columns='state_expected', inplace=True)\n",
    "X_val.drop(columns='state_expected', inplace=True)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "X_val.fillna(0, inplace=True)\n",
    "print(y_val.value_counts())\n",
    "\n",
    "# keep not standardized data in X_train_not_scaled, X_test_not_scaled, X_val_not_scaled\n",
    "X_train_not_scaled = X_train.copy()\n",
    "X_test_not_scaled = X_test.copy()\n",
    "X_val_not_scaled = X_val.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=100, multi_class='ovr', C=0.1)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_not_scaled.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "log_reg.classes_ = state_expected_keys.state.values\n",
    "log_reg.classes_\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)\n",
    "plt.xlabel('Predikované')\n",
    "plt.ylabel('Aktuálne')\n",
    "plt.title('Konfúzna matica LogReg')\n",
    "plt.legend([f'Presnosť: {accuracy:.2f}'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Parameter grid for RandomizedSearch\n",
    "param_dist_lgb = {\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'min_child_samples': [20, 50, 100],\n",
    "    'min_child_weight': [1e-3, 1e-2, 1e-1, 1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 1e-1, 1, 2],\n",
    "    'reg_lambda': [0, 1e-1, 1, 2],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "random_search_lgb = RandomizedSearchCV(lgb_model, param_distributions=param_dist_lgb, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_lgb_model = random_search_lgb.best_estimator_\n",
    "best_lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_lgb = best_lgb_model.predict(X_val)\n",
    "val_accuracy_lgb = accuracy_score(y_val, val_pred_lgb)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_lgb.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('LightGBM', val_accuracy_lgb, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for LightGBM: {val_accuracy_lgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_imp = best_lgb_model.feature_importances_\n",
    "\n",
    "# Creating a DataFrame to hold the feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train_not_scaled.columns, 'Importance': feature_imp})\n",
    "\n",
    "# Sort the DataFrame to find the most important features\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Plotting feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance for LightGBM Model')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to have the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, val_pred_lgb)\n",
    "\n",
    "# Trasnform classes from numerical to names in someway\n",
    "predicted_names = [mapping_dict[label] for label in best_lgb_model.classes_]\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predikované')\n",
    "plt.ylabel('Aktuálne')\n",
    "plt.title(f'Konfúzna matica LightGBM, Val Acc: {val_accuracy_lgb:.2f}')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_lgb\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia LightGBM'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGB Classifier\n",
    "param_dist_xgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0.01, 0.1, 1],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "random_search_xgb = RandomizedSearchCV(lgb_model, param_distributions=param_dist_xgb, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_xgb_model = random_search_xgb.best_estimator_\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_xgb = best_xgb_model.predict(X_val)\n",
    "val_accuracy_xgb = accuracy_score(y_val, val_pred_xgb)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_xgb.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('LightGBM', val_accuracy_xgb, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for XGBM: {val_accuracy_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, val_pred_xgb)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_xgb_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix XGBoost, Val Acc: {val_accuracy_xgb:.2f}')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_xgb\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia XGB'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN model for classification using the df dataset print the accuracy of the model\n",
    "param_distributions_knn = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "knn_model = KNeighborsClassifier()\n",
    "random_search_knn = RandomizedSearchCV(knn_model, param_distributions=param_distributions_knn, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_knn_model = random_search_knn.best_estimator_\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_knn = best_knn_model.predict(X_val)\n",
    "val_accuracy_knn = accuracy_score(y_val, val_pred_knn)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "params_str = json.dumps(random_search_knn.best_params_)\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('KNN', val_accuracy_knn, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for KNN: {val_accuracy_knn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, val_pred_xgb)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_knn_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix KNN, Val Acc: {val_accuracy_xgb:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_knn\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia KNN'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "rf_model = RandomForestClassifier()\n",
    "random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_distributions_rf, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_rf = best_rf_model.predict(X_val)\n",
    "val_accuracy_rf = accuracy_score(y_val, val_pred_rf)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_rf.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('KNN', val_accuracy_rf, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for RandomForest: {val_accuracy_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_rf_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix RandomForest, Val Acc: {val_accuracy_rf:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_rf\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia RF'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree model for classification using the df dataset print the accuracy of the model\n",
    "param_distributions_dt = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "random_search_dt = RandomizedSearchCV(dt_model, param_distributions=param_distributions_dt, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_dt_model = random_search_dt.best_estimator_\n",
    "best_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_dt = best_dt_model.predict(X_val)\n",
    "val_accuracy_dt = accuracy_score(y_val, val_pred_dt)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_dt.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('DecisionTree', val_accuracy_dt, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for DecisionTree: {val_accuracy_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_dt_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix DecisionTree, Val Acc: {val_accuracy_dt:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_dt\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia DT'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM\n",
    "param_distributions_svm = {\n",
    "    'C': [1],\n",
    "    'gamma': ['scale'],\n",
    "    'kernel': ['rbf'],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "svm_model = SVC()\n",
    "random_search_svm = RandomizedSearchCV(svm_model, param_distributions=param_distributions_svm, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_svm_model = random_search_svm.best_estimator_\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_svm = best_svm_model.predict(X_val)\n",
    "val_accuracy_svm = accuracy_score(y_val, val_pred_svm)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_svm.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('SVM', val_accuracy_svm, params_str, '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for SVM: {val_accuracy_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_svm_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix SVM, Val Acc: {val_accuracy_svm:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_svm\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia SVM'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multilayer perceptron model for classification using the df dataset print the accuracy of the model\n",
    "param_distributions_mlp = {\n",
    "    'hidden_layer_sizes': [(100,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.001],\n",
    "    'learning_rate': ['constant'],\n",
    "}\n",
    "\n",
    "# Setup the model\n",
    "mlp_model = MLPClassifier()\n",
    "random_search_mlp = RandomizedSearchCV(mlp_model, param_distributions=param_distributions_mlp, \n",
    "    n_iter=1,  \n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "random_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Pick the best model\n",
    "best_mlp_model = random_search_mlp.best_estimator_\n",
    "best_mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = best_mlp_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "display(print(\"Accuracy:\", accuracy))\n",
    "\n",
    "# For LightGBM example\n",
    "val_pred_mlp = best_mlp_model.predict(X_val)\n",
    "val_accuracy_mlp = accuracy_score(y_val, val_pred_mlp)\n",
    "\n",
    "# Save the model name, validation score, hyperparameters and train/test/validation distribution to the database\n",
    "params_str = json.dumps(random_search_mlp.best_params_)\n",
    "conn = sqlite3.connect('model_results.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"INSERT INTO model_results VALUES (?, ?, ?, ?)\", ('MLP', val_accuracy_mlp, params_str,  '80/20/1'))\n",
    "conn.commit()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f'Validation Accuracy for MLP: {val_accuracy_mlp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Trasnform classes from numerical to names using state_expected_keys\n",
    "predicted_names = [mapping_dict[label] for label in best_mlp_model.classes_]\n",
    "\n",
    "# Show confnusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=predicted_names, yticklabels=predicted_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion matrix MLP, Val Acc: {val_accuracy_mlp:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfer = y_val.reset_index(drop=True).to_frame()\n",
    "dfer['time'] = X_val_not_scaled.time_normalized.reset_index(drop=True)\n",
    "dfer['state'] = val_pred_mlp\n",
    "dfer['yval'] = y_val.reset_index(drop=True)\n",
    "dfer['range'] = dfer.index\n",
    "dfer['good_prediction'] = (dfer.state == dfer.yval).astype(int)\n",
    "\n",
    "# Assuming 'time' is your feature and 'good_prediction' is the binary outcome\n",
    "X = dfer[['time']].values  # Features need to be 2D for scikit-learn\n",
    "y = dfer['good_prediction'].values  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Generate a range of x-values for predictions\n",
    "x_range = np.linspace(dfer['time'].min(), dfer['time'].max(), 300).reshape(-1, 1)\n",
    "\n",
    "# Predict probabilities for the generated x-values\n",
    "y_pred = model.predict_proba(x_range)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, ggtitle\n",
    "\n",
    "# Original data points\n",
    "original_plot = (ggplot(dfer, aes(x='time', y='good_prediction', color='good_prediction')) +\n",
    "                 geom_point() +\n",
    "                 ggtitle('Predikcia MLP'))\n",
    "\n",
    "# Create a DataFrame for the logistic curve\n",
    "logistic_df = pd.DataFrame({'time': x_range.flatten(), 'probability': y_pred})\n",
    "\n",
    "# Add the logistic curve to the plot\n",
    "logistic_plot = (original_plot +\n",
    "                 geom_line(data=logistic_df, mapping=aes(x='time', y='probability'), color='red'))\n",
    "\n",
    "logistic_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA, LSTM, GRU, CNN-LSTM, TCN, transformers Training Time, Feature Importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
